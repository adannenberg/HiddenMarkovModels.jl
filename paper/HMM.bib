@misc{antonelloHMMGradientsJlEnables2021,
  title = {{{HMMGradients}}.Jl: {{Enables}} Computing the Gradient of the Parameters of {{Hidden Markov Models}} ({{HMMs}})},
  shorttitle = {Idiap/{{HMMGradients}}.Jl},
  author = {Antonello, Niccol{\`o}},
  year = {2021},
  month = jun,
  doi = {10.5281/zenodo.4454565},
  urldate = {2023-09-12},
  howpublished = {Zenodo},
  keywords = {hmm},
  file = {/Users/guillaume/Zotero/storage/PEFYSLF7/4906644.html}
}

@inproceedings{bengioInputOutputHMM1994,
  title = {An {{Input Output HMM Architecture}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Frasconi, Paolo},
  year = {1994},
  volume = {7},
  publisher = {{MIT Press}},
  urldate = {2023-03-12},
  abstract = {We  introduce  a  recurrent  architecture  having  a  modular structure  and we formulate a training procedure based on the EM  algorithm.  The resulting model has similarities to hidden  Markov models, but  supports  recurrent  networks  processing style and allows  to exploit  the supervised  learning paradigm while using maximum likelihood  estimation.},
  keywords = {hmm,thesis},
  file = {/Users/guillaume/Zotero/storage/68UNNYP2/Bengio_Frasconi_1994_An Input Output HMM Architecture.pdf}
}

@article{besanconDistributionsJlDefinition2021,
  title = {Distributions{{.jl}}: {{Definition}} and {{Modeling}} of {{Probability Distributions}} in the {{JuliaStats Ecosystem}}},
  shorttitle = {Distributions.Jl},
  author = {Besan{\c c}on, Mathieu and Papamarkou, Theodore and Anthoff, David and Arslan, Alex and Byrne, Simon and Lin, Dahua and Pearson, John},
  year = {2021},
  month = jul,
  journal = {Journal of Statistical Software},
  volume = {98},
  pages = {1--30},
  issn = {1548-7660},
  doi = {10.18637/jss.v098.i16},
  urldate = {2022-09-19},
  abstract = {Random variables and their distributions are a central part in many areas of statistical methods. The Distributions.jl package provides Julia users and developers tools for working with probability distributions, leveraging Julia features for their intuitive and flexible manipulation, while remaining highly efficient through zero-cost abstractions.},
  copyright = {Copyright (c) 2021 Mathieu Besan{\c c}on, Theodore Papamarkou, David Anthoff, Alex Arslan, Simon Byrne, Dahua Lin, John Pearson},
  langid = {english},
  keywords = {hmm,thesis},
  file = {/Users/guillaume/Zotero/storage/FZ5V2QNZ/Besancon et al_2021_Distributions.pdf}
}

@article{bezansonJuliaFreshApproach2017,
  title = {Julia: {{A Fresh Approach}} to {{Numerical Computing}}},
  shorttitle = {Julia},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  pages = {65--98},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/141000671},
  urldate = {2022-12-03},
  langid = {english},
  keywords = {bootstrap,hmm,inferopt,povar,thesis,viva},
  file = {/Users/guillaume/Zotero/storage/YWLISSFK/Bezanson et al_2017_Julia.pdf}
}

@book{cappeInferenceHiddenMarkov2005,
  title = {Inference in {{Hidden Markov Models}}},
  author = {Capp{\'e}, Olivier and Moulines, Eric and Ryd{\'e}n, Tobias},
  year = {2005},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/0-387-28982-8},
  urldate = {2022-12-03},
  googlebooks = {4d\_oEYn8Fl0C},
  isbn = {978-0-387-40264-2 978-0-387-28982-3},
  langid = {english},
  keywords = {hmm,povar,thesis},
  file = {/Users/guillaume/Zotero/storage/2HYZE7ZD/Cappé et al_2005_Inference in Hidden Markov Models.pdf;/Users/guillaume/Zotero/storage/QRNV9CL8/Cappé et al. - 2006 - Inference in Hidden Markov Models.pdf}
}

@misc{changDynamaxStateSpace2024,
  title = {Dynamax:  {{State Space Models}} Library in {{JAX}}},
  author = {Chang, Peter and {Harper-Donnelly}, Giles and Kara, Aleyna and Li, Xinglong and Linderman, Scott and Murphy, Kevin},
  year = {2024},
  month = feb,
  urldate = {2024-02-22},
  abstract = {State Space Models library in JAX},
  copyright = {MIT},
  howpublished = {Probabilistic machine learning},
  keywords = {hmm}
}

@misc{chenRobustBenchmarkingNoisy2016,
  title = {Robust Benchmarking in Noisy Environments},
  author = {Chen, Jiahao and Revels, Jarrett},
  year = {2016},
  month = aug,
  number = {arXiv:1608.04295},
  eprint = {1608.04295},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1608.04295},
  urldate = {2024-02-29},
  abstract = {We propose a benchmarking strategy that is robust in the presence of timer error, OS jitter and other environmental fluctuations, and is insensitive to the highly nonideal statistics produced by timing measurements. We construct a model that explains how these strongly nonideal statistics can arise from environmental fluctuations, and also justifies our proposed strategy. We implement this strategy in the BenchmarkTools Julia package, where it is used in production continuous integration (CI) pipelines for developing the Julia language and its ecosystem.},
  archiveprefix = {arxiv},
  keywords = {hmm},
  file = {/Users/guillaume/Zotero/storage/E8N8ZJH3/Chen and Revels - 2016 - Robust benchmarking in noisy environments.pdf;/Users/guillaume/Zotero/storage/ALUPXBPZ/1608.html}
}

@phdthesis{dalleMachineLearningCombinatorial2022,
  title = {Machine Learning and Combinatorial Optimization Algorithms, with Applications to Railway Planning},
  author = {Dalle, Guillaume},
  year = {2022},
  month = dec,
  abbr = {Dissertation},
  abstract = {This thesis investigates the frontier between machine learning and combinatorial optimization, two active areas of applied mathematics research. We combine theoretical insights with efficient algorithms, and develop several open source Julia libraries. Inspired by a collaboration with the Soci{\'e}t{\'e} nationale des chemins de fer fran{\c c}ais (SNCF), we study high-impact use cases from the railway world: train failure prediction, delay propagation, and track allocation.In Part I, we provide mathematical background and describe software implementations for various tools that will be needed later on: implicit differentiation, temporal point processes, Hidden Markov Models and Multi-Agent Path Finding. Our publicly-available code fills a void in the Julia package ecosystem, aiming at ease of use without compromising on performance.In Part II, we highlight theoretical contributions related to both statistics and decision-making. We consider a Vector AutoRegressive process with partial observations, and prove matching upper and lower bounds on the estimation error. We unify and extend the state of the art for combinatorial optimization layers in deep learning, gathering various approaches in a Julia library called InferOpt.jl. We also seek to differentiate through multi-objective optimization layers, which leads to a novel theory of lexicographic convex analysis.In Part III, these mathematical and algorithmic foundations come together to tackle railway problems. We design a hierarchical model of train failures, propose a graph-based framework for delay propagation, and suggest new avenues for track allocation, with the Flatland challenge as a testing ground.},
  collaborator = {Meunier, Fr{\'e}d{\'e}ric and De Castro, Yohann and Parmentier, Axel},
  copyright = {Licence Etalab},
  langid = {english},
  pdf = {https://pastel.archives-ouvertes.fr/tel-04053322},
  school = {{\'E}cole des Ponts ParisTech},
  keywords = {cv,hmm,website},
  file = {/Users/guillaume/Zotero/storage/CEVJMUP4/Dalle - Machine learning and combinatorial optimization al.pdf}
}

@article{danischMakieJlFlexible2021,
  title = {Makie.Jl: {{Flexible}} High-Performance Data Visualization for {{Julia}}},
  shorttitle = {Makie.Jl},
  author = {Danisch, Simon and Krumbiegel, Julius},
  year = {2021},
  month = sep,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {65},
  pages = {3349},
  issn = {2475-9066},
  doi = {10.21105/joss.03349},
  urldate = {2024-02-29},
  abstract = {Danisch et al., (2021). Makie.jl: Flexible high-performance data visualization for Julia. Journal of Open Source Software, 6(65), 3349, https://doi.org/10.21105/joss.03349},
  langid = {english},
  keywords = {hmm},
  file = {/Users/guillaume/Zotero/storage/T4W5S92V/Danisch and Krumbiegel - 2021 - Makie.jl Flexible high-performance data visualiza.pdf}
}

@misc{hmmlearndevelopersHmmlearnHiddenMarkov2023,
  title = {Hmmlearn: {{Hidden Markov Models}} in {{Python}}, with Scikit-Learn like {{API}}},
  author = {{hmmlearn developers}},
  year = {2023},
  urldate = {2023-09-12},
  abstract = {Hidden Markov Models in Python, with scikit-learn like API},
  copyright = {BSD-3-Clause},
  howpublished = {hmmlearn},
  keywords = {hmm}
}

@misc{mouchetHMMBaseJlHidden2023,
  title = {{{HMMBase}}.Jl:  {{Hidden Markov Models}} for {{Julia}}},
  author = {Mouchet, Maxime},
  year = {2023},
  urldate = {2023-09-12},
  abstract = {Hidden Markov Models for Julia.},
  copyright = {MIT},
  keywords = {hmm}
}

@book{murphyProbabilisticMachineLearning2023,
  title = {Probabilistic Machine Learning: Advanced Topics},
  author = {Murphy, Kevin P.},
  year = {2023},
  publisher = {{The MIT Press}},
  keywords = {hmm,todo},
  file = {/Users/guillaume/Zotero/storage/DXSP888K/Murphy - 2023 - Probabilistic machine learning advanced topics.pdf;/Users/guillaume/Zotero/storage/XMNWZH35/supp2.pdf}
}

@inproceedings{ondelGPUAcceleratedForwardBackwardAlgorithm2022,
  title = {{{GPU-Accelerated Forward-Backward Algorithm}} with {{Application}} to {{Lattice-Free MMI}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ondel, Lucas and {Lam-Yee-Mui}, L{\'e}a-Marie and Kocour, Martin and Corro, Caio Filippo and Burget, Luk{\'a}s},
  year = {2022},
  month = may,
  pages = {8417--8421},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746824},
  urldate = {2024-03-01},
  abstract = {We propose to express the forward-backward algorithm in terms of operations between sparse matrices in a specific semiring. This new perspective naturally leads to a GPU-friendly algorithm which is easy to implement in Julia or any programming languages with native support of semiring algebra. We use this new implementation to train a TDNN with the LF-MMI objective function and we compare the training time of our system with PyChain---a recently introduced C++/CUDA implementation of the LF-MMI loss. Our implementation is about two times faster while not having to use any approximation such as the "leaky-HMM".},
  keywords = {hmm},
  file = {/Users/guillaume/Zotero/storage/PCXZVCW3/Ondel et al. - 2022 - GPU-Accelerated Forward-Backward Algorithm with Ap.pdf;/Users/guillaume/Zotero/storage/I5R9E3DD/9746824.html}
}

@article{qinDirectOptimizationApproach2000,
  title = {A {{Direct Optimization Approach}} to {{Hidden Markov Modeling}} for {{Single Channel Kinetics}}},
  author = {Qin, Feng and Auerbach, Anthony and Sachs, Frederick},
  year = {2000},
  month = oct,
  journal = {Biophysical Journal},
  volume = {79},
  number = {4},
  pages = {1915--1927},
  issn = {0006-3495},
  doi = {10.1016/S0006-3495(00)76441-1},
  urldate = {2022-08-06},
  abstract = {Hidden Markov modeling (HMM) provides an effective approach for modeling single channel kinetics. Standard HMM is based on Baum's reestimation. As applied to single channel currents, the algorithm has the inability to optimize the rate constants directly. We present here an alternative approach by considering the problem as a general optimization problem. The quasi-Newton method is used for searching the likelihood surface. The analytical derivatives of the likelihood function are derived, thereby maximizing the efficiency of the optimization. Because the rate constants are optimized directly, the approach has advantages such as the allowance for model constraints and the ability to simultaneously fit multiple data sets obtained at different experimental conditions. Numerical examples are presented to illustrate the performance of the algorithm. Comparisons with Baum's reestimation suggest that the approach has a superior convergence speed when the likelihood surface is poorly defined due to, for example, a low signal-to-noise ratio or the aggregation of multiple states having identical conductances.},
  langid = {english},
  keywords = {hmm,thesis,viva},
  file = {/Users/guillaume/Zotero/storage/EPDNRHUX/Qin et al. - 2000 - A Direct Optimization Approach to Hidden Markov Mo.pdf;/Users/guillaume/Zotero/storage/6C5WNKEU/S0006349500764411.html}
}

@article{rabinerTutorialHiddenMarkov1989,
  title = {A Tutorial on Hidden {{Markov}} Models and Selected Applications in Speech Recognition},
  author = {Rabiner, L.R.},
  year = {1989},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {1558-2256},
  doi = {10/cswph2},
  abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described.{$<>$}},
  keywords = {done,hmm,thesis,viva},
  file = {/Users/guillaume/Zotero/storage/A68ILRMJ/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/Users/guillaume/Zotero/storage/BEJEKP4E/Rabiner_1989_A tutorial on hidden Markov models and selected applications in speech.pdf;/Users/guillaume/Zotero/storage/5BHQF7ME/18626.html}
}

@misc{rowleyLogarithmicNumbersJlLogarithmic2023,
  title = {{{LogarithmicNumbers}}.Jl: {{A}} Logarithmic Number System for {{Julia}}.},
  author = {Rowley, Christopher},
  year = {2023},
  month = may,
  urldate = {2023-09-12},
  abstract = {A logarithmic number system for Julia.},
  copyright = {MIT},
  keywords = {hmm}
}

@misc{rowleyPythonCallJlPython2022,
  title = {{{PythonCall}}.Jl: {{Python}} and {{Julia}} in Harmony},
  author = {Rowley, Christopher},
  year = {2022},
  urldate = {2024-02-29},
  abstract = {Python and Julia in harmony.},
  copyright = {MIT},
  howpublished = {JuliaPy},
  keywords = {hmm}
}

@article{schreiberPomegranateFastFlexible2018,
  title = {Pomegranate: {{Fast}} and {{Flexible Probabilistic Modeling}} in {{Python}}},
  shorttitle = {Pomegranate},
  author = {Schreiber, Jacob},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {164},
  pages = {1--6},
  issn = {1533-7928},
  urldate = {2019-05-16},
  keywords = {hmm},
  file = {/Users/guillaume/Zotero/storage/QKMY8X8M/Schreiber_2018_pomegranate.pdf;/Users/guillaume/Zotero/storage/9GWGT5RK/17-636.html}
}

@misc{whiteJuliaDiffChainRulesJl2022,
  title = {{{JuliaDiff}}/{{ChainRules}}{{.jl}}: V1.44.7},
  shorttitle = {{{JuliaDiff}}/{{ChainRules}}.Jl},
  author = {White, Frames Catherine and Abbott, Michael and Zgubic, Miha and Revels, Jarrett and Axen, Seth and Arslan, Alex and Schaub, Simeon and Robinson, Nick and Yingbo Ma and Gaurav Dhingra and Tebbutt, Will and Heim, Niklas and Widmann, David and Rosemberg, Andrew David Werner and Schmitz, Niklas and Rackauckas, Christopher and Heintzmann, Rainer and Frankschae and Noack, Andreas and Lucibello, Carlo and Fischer, Keno and Robson, Alex and Cossio and Ling, Jerry and MattBrzezinski and Finnegan, Rory and Zhabinski, Andrei and Wennberg, Daniel and Besan{\c c}on, Mathieu and Vertechi, Pietro},
  year = {2022},
  month = oct,
  doi = {10.5281/ZENODO.4754896},
  urldate = {2022-10-13},
  abstract = {ChainRules v1.44.7 Diff since v1.44.6 {$<$}strong{$>$}Closed issues:{$<$}/strong{$>$} cat with Val tuple dims fails (\#678) {$<$}strong{$>$}Merged pull requests:{$<$}/strong{$>$} Fix for ChainRulesCore \#586 (\#675) (@rofinn) fix cat rrule (\#679) (@cossio)},
  copyright = {Open Access},
  howpublished = {Zenodo},
  keywords = {#nosource,hmm,inferopt,thesis}
}
