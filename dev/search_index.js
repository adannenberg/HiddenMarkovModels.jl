var documenterSearchIndex = {"docs":
[{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [HiddenMarkovModels]","category":"page"},{"location":"api/#HiddenMarkovModels.HMMs","page":"API reference","title":"HiddenMarkovModels.HMMs","text":"HMMs\n\nAlias for the module HiddenMarkovModels.\n\n\n\n\n\n","category":"module"},{"location":"api/#HiddenMarkovModels.HiddenMarkovModels","page":"API reference","title":"HiddenMarkovModels.HiddenMarkovModels","text":"HiddenMarkovModels\n\nA Julia package for HMM modeling, simulation, inference and learning.\n\n\n\n\n\n","category":"module"},{"location":"api/#HiddenMarkovModels.ForwardBackwardStorage","page":"API reference","title":"HiddenMarkovModels.ForwardBackwardStorage","text":"ForwardBackwardStorage{R}\n\nFields\n\nα::Matrix{R}: forward variables α[i, t]\nc::Vector{R}: forward variable inverse normalizations c[t]\nβ::Matrix{R}: backward variables β[i, t]\nBβ::Matrix{R}: auxiliary backward variables multiplied by the likelihoods Bβ[i, t] = B[i, t] * β[i, t]\nγ::Matrix{R}: one-state marginals γ[i, t]\nξ::Array{R,3}: two-state marginals ξ[i, j, t]\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.HMM","page":"API reference","title":"HiddenMarkovModels.HMM","text":"HMM\n\nAlias for the struct HiddenMarkovModel.\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.HiddenMarkovModel","page":"API reference","title":"HiddenMarkovModels.HiddenMarkovModel","text":"HiddenMarkovModel{SP<:StateProcess,OP<:ObservationProcess}\n\nCombination of a state and an observation process, amenable to simulation, inference and learning.\n\nFields\n\nstate_process::SP\nobs_process::OP\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.ObservationProcess","page":"API reference","title":"HiddenMarkovModels.ObservationProcess","text":"ObservationProcess\n\nAbstract type for the observation part of an HMM.\n\nRequired methods\n\nBase.length(op)\ndistribution(op, i)\n\nOptional methods\n\nreestimate!(op, obs_seq, γ)\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.StandardObservationProcess","page":"API reference","title":"HiddenMarkovModels.StandardObservationProcess","text":"StandardObservationProcess{D} <: ObservationProcess\n\nFields\n\ndistributions::AbstractVector{D}: one distribution per state\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.StandardStateProcess","page":"API reference","title":"HiddenMarkovModels.StandardStateProcess","text":"StandardStateProcess <: StateProcess\n\nFields\n\np::AbstractVector: initial distribution\nA::AbstractMatrix: transition matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.StateProcess","page":"API reference","title":"HiddenMarkovModels.StateProcess","text":"StateProcess\n\nAbstract type for the state part of an HMM.\n\nRequired methods\n\nBase.length(sp)\ninitial_distribution(sp)\ntransition_matrix(sp)\n\nOptional methods\n\nreestimate!(sp, p_count, A_count)\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.rand-Tuple{HiddenMarkovModel, Integer}","page":"API reference","title":"Base.rand","text":"rand(hmm, T)\n\nSimulate an HMM for T time steps.\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG, HiddenMarkovModel, Integer}","page":"API reference","title":"Base.rand","text":"rand(rng, hmm, T)\n\nSimulate an HMM for T time steps with a specified rng.\n\n\n\n\n\n","category":"method"},{"location":"api/#HiddenMarkovModels.baum_welch-Tuple{HiddenMarkovModel, Any}","page":"API reference","title":"HiddenMarkovModels.baum_welch","text":"baum_welch(hmm_init, obs_seqs; max_iterations, rtol)\n\nApply the Baum-Welch algorithm to estimate the parameters of an HMM on multiple observation sequences, and return a tuple (hmm, logL_evolution).\n\n\n\n\n\n","category":"method"},{"location":"api/#HiddenMarkovModels.forward_backward-Tuple{HiddenMarkovModel, Any}","page":"API reference","title":"HiddenMarkovModels.forward_backward","text":"forward_backward(hmm, obs_seq)\n\nApply the forward-backward algorithm to estimate the posterior state marginals of an HMM, and return a ForwardBackwardStorage object.\n\n\n\n\n\n","category":"method"},{"location":"api/#HiddenMarkovModels.viterbi-Tuple{HiddenMarkovModel, Any}","page":"API reference","title":"HiddenMarkovModels.viterbi","text":"viterbi(hmm, obs_seq)\n\nApply the Viterbi algorithm to compute the most likely sequence of states of an HMM, and return it as a vector of integers.\n\n\n\n\n\n","category":"method"},{"location":"notations/#Notations","page":"Notations","title":"Notations","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"Our whole package is based on the famous paper by Rabiner (1989), A tutorial on hidden Markov models and selected applications in speech recognition.","category":"page"},{"location":"notations/#State-process","page":"Notations","title":"State process","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"sp or state_process: a StateProcess\np: initial_distribution (vector of state probabilities)\nA: transition_matrix (matrix of transition probabilities)\nstate_seq: a sequence of states (vector of integers)","category":"page"},{"location":"notations/#Observation-process","page":"Notations","title":"Observation process","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"op or obs_process: an ObservationProcess\n(log)b: vector of observation (log)likelihoods by state for a single observation\n(log)B: matrix of observation (log)likelihoods by state for a sequence of observations\nobs_seq: a sequence of observations\nobs_seqs: several sequences of observations","category":"page"},{"location":"notations/#Forward-backward","page":"Notations","title":"Forward backward","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"α: forward variables\nc: forward variable inverse normalizations\nβ: backward variables\nγ: one-state marginals\nξ: two-state marginals\nlogL: posterior loglikelihood of a sequence of observations","category":"page"},{"location":"#HiddenMarkovModels.jl","page":"Home","title":"HiddenMarkovModels.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Julia package for HMM modeling, simulation, inference and learning.","category":"page"},{"location":"#Main-features","page":"Home","title":"Main features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Efficiency\nallocation-free versions of core functions\nlinear algebra subroutines\nGeneric states\ndense or sparse\nwith or without prior\nGeneric observations\nDistributions.jl\nMeasureTheory.jl\nanything that follows DensityInterface.jl\nGeneric number types\nfloating point precision\nLogarithmicNumbers.jl\nAutomatic differentiation of parameters\nin forward mode\nin reverse mode","category":"page"},{"location":"#Inspirations","page":"Home","title":"Inspirations","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"HMMBase.jl\nHMMGradients.jl\nControlledHiddenMarkovModels.jl","category":"page"}]
}
