var documenterSearchIndex = {"docs":
[{"location":"benchmarks/#Benchmarks","page":"Benchmarks","title":"Benchmarks","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The test case is an HMM with diagonal multivariate normal observations. We compare the following packages:","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"HiddenMarkovModels.jl (abbreviated to HMMs.jl)\nHMMBase.jl\nhmmlearn\npomegranate","category":"page"},{"location":"benchmarks/#Results","page":"Benchmarks","title":"Results","text":"","category":"section"},{"location":"benchmarks/#Single-sequence","page":"Benchmarks","title":"Single sequence","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Full benchmark logs: results_single_sequence.csv.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Logdensity single sequence benchmark)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Viterbi single sequence benchmark)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Forward-backward single sequence benchmark)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Baum-Welch single sequence benchmark)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Here, pomegranate is not included because it is much slower on very small inputs.","category":"page"},{"location":"benchmarks/#Multiple-sequences","page":"Benchmarks","title":"Multiple sequences","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Full benchmark logs: results_multiple_sequences.csv.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Logdensity single sequence benchmark)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"(Image: Baum-Welch single sequence benchmark)","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Here, HMMBase.jl is not included because it does not support multiple sequences.","category":"page"},{"location":"benchmarks/#Reproducibility","page":"Benchmarks","title":"Reproducibility","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"These benchmarks were generated in the following environment: setup.txt.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"If you want to run them on your machine:","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Clone the HiddenMarkovModels.jl repository\nOpen a Julia REPL at the root\nRun the following commands\ninclude(\"benchmark/run_benchmarks.jl\")\ninclude(\"docs/process_benchmarks.jl\")","category":"page"},{"location":"benchmarks/#Remarks","page":"Benchmarks","title":"Remarks","text":"","category":"section"},{"location":"benchmarks/#Allocations","page":"Benchmarks","title":"Allocations","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"A major bottleneck of performance in Julia is memory allocations. The benchmarks for HMMs.jl thus employ a custom implementation of diagonal multivariate normals, which is entirely allocation-free.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"This partly explains the performance gap with HMMBase.jl as the dimension D grows beyond 1. Such a trick is also possible with HMMBase.jl, but slightly more demanding since it requires subtyping Distribution from Distributions.jl, instead of just implementing DensityInterface.jl. We might do it in future benchmarks.","category":"page"},{"location":"benchmarks/#Parallelism","page":"Benchmarks","title":"Parallelism","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"The packages we include have different approaches to parallelism, which can bias the evaluation in complex ways:","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"Package States N Observations D Sequences K\nHMMs.jl LinearAlgebra[2] depends[2] Threads[1]\nHMMBase.jl - depends[2] -\nhmmlearn NumPy[2] NumPy[2] NumPy[2]\nhmmlearn PyTorch[3] PyTorch[3] PyTorch[3]","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"[1]: possibly affected by JULIA_NUM_THREADS","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"[2]: possibly affected by OPENBLAS_NUM_THREADS","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"[3]: possibly affected by MKL_NUM_THREADS","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"For a fairer comparison, we set JULIA_NUM_THREADS=1, even though it robs HMMs.jl of its parallel speedup on multiple sequences.","category":"page"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"In addition, OpenBLAS threads have negative interactions with Julia threads. To overcome this obstacle, we run the Julia benchmarks (and only those) with OPENBLAS_NUM_THREADS=1.","category":"page"},{"location":"benchmarks/#Acknowledgements","page":"Benchmarks","title":"Acknowledgements","text":"","category":"section"},{"location":"benchmarks/","page":"Benchmarks","title":"Benchmarks","text":"A big thank you to Maxime Mouchet and Jacob Schreiber, the respective lead devs of HMMBase.jl and pomegranate, for their help.","category":"page"},{"location":"api/#API-reference","page":"API reference","title":"API reference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"HiddenMarkovModels\nHMMs","category":"page"},{"location":"api/#HiddenMarkovModels","page":"API reference","title":"HiddenMarkovModels","text":"HiddenMarkovModels\n\nA Julia package for HMM modeling, simulation, inference and learning.\n\n\n\n\n\n","category":"module"},{"location":"api/#HiddenMarkovModels.HMMs","page":"API reference","title":"HiddenMarkovModels.HMMs","text":"HMMs\n\nAlias for the module HiddenMarkovModels.\n\n\n\n\n\n","category":"module"},{"location":"api/#Types","page":"API reference","title":"Types","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"HiddenMarkovModel\nHMM\nHMMs.StateProcess\nHMMs.StandardStateProcess\nHMMs.ObservationProcess\nHMMs.StandardObservationProcess","category":"page"},{"location":"api/#HiddenMarkovModels.HiddenMarkovModel","page":"API reference","title":"HiddenMarkovModels.HiddenMarkovModel","text":"HiddenMarkovModel{SP<:StateProcess,OP<:ObservationProcess}\n\nCombination of a state and an observation process, amenable to simulation, inference and learning.\n\nFields\n\nstate_process::SP\nobs_process::OP\n\nBoth fields are considered part of the API, which is why there are no accessors.\n\nApplicable methods\n\ninitial_distribution(hmm)\ntransition_matrix(hmm)\nobs_distribution(hmm, i)\nrand([rng,] hmm, T)\nlogdensityof(hmm, obs_seq)\nviterbi(hmm, obs_seq)\nforward_backward(hmm, obs_seq)\nbaum_welch(hmm, obs_seq)\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.HMM","page":"API reference","title":"HiddenMarkovModels.HMM","text":"HMM\n\nAlias for the struct HiddenMarkovModel.\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.StateProcess","page":"API reference","title":"HiddenMarkovModels.StateProcess","text":"StateProcess\n\nAbstract type for the state part of an HMM.\n\nRequired methods\n\nlength(sp)\ninitial_distribution(sp)\ntransition_matrix(sp)\n\nOptional methods\n\nfit!(sp, p_count, A_count)\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.StandardStateProcess","page":"API reference","title":"HiddenMarkovModels.StandardStateProcess","text":"StandardStateProcess <: StateProcess\n\nFields\n\np::AbstractVector: initial distribution\nA::AbstractMatrix: transition matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.ObservationProcess","page":"API reference","title":"HiddenMarkovModels.ObservationProcess","text":"ObservationProcess\n\nAbstract type for the observation part of an HMM.\n\nRequired methods\n\nlength(op)\nobs_distribution(op, i)\n\nOptional methods\n\nfit!(op, obs_seq, Î³)\n\n\n\n\n\n","category":"type"},{"location":"api/#HiddenMarkovModels.StandardObservationProcess","page":"API reference","title":"HiddenMarkovModels.StandardObservationProcess","text":"StandardObservationProcess{D} <: ObservationProcess\n\nFields\n\ndistributions::AbstractVector{D}: one distribution per state\n\n\n\n\n\n","category":"type"},{"location":"api/#Basics","page":"API reference","title":"Basics","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"length\nrand\ninitial_distribution\ntransition_matrix\nobs_distribution","category":"page"},{"location":"api/#Base.length","page":"API reference","title":"Base.length","text":"length(sp::StateProcess)\n\nReturn the number of states of sp.\n\n\n\n\n\nlength(op::ObservationProcess)\n\nReturn the number of states of op.\n\n\n\n\n\nlength(hmm::HMM)\n\nReturn the number of states of hmm.\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.rand","page":"API reference","title":"Base.rand","text":"rand(rng, hmm, T)\n\nSimulate an HMM for T time steps with a specified rng.\n\n\n\n\n\nrand(hmm, T)\n\nSimulate an HMM for T time steps.\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.initial_distribution","page":"API reference","title":"HiddenMarkovModels.initial_distribution","text":"initial_distribution(sp::StateProcess)\n\nReturn the initial state probabilities of sp.\n\n\n\n\n\ninitial_distribution(hmm::HMM)\n\nReturn the initial state probabilities of hmm.\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.transition_matrix","page":"API reference","title":"HiddenMarkovModels.transition_matrix","text":"transition_matrix(sp::StateProcess)\n\nReturn the state transition probabilities of sp.\n\n\n\n\n\ntransition_matrix(hmm::HMM)\n\nReturn the state transition probabilities of hmm.\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.obs_distribution","page":"API reference","title":"HiddenMarkovModels.obs_distribution","text":"obs_distribution(op::ObservationProcess, i)\n\nReturn the observation distribution of op associated with state i.\n\n\n\n\n\nobs_distribution(hmm::HMM, i)\n\nReturn the observation distribution of hmm associated with state i.\n\n\n\n\n\n","category":"function"},{"location":"api/#Inference","page":"API reference","title":"Inference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"logdensityof\nviterbi\nforward_backward","category":"page"},{"location":"api/#DensityInterface.logdensityof","page":"API reference","title":"DensityInterface.logdensityof","text":"DensityInterface.logdensityof(hmm, obs_seq)\n\nApply the forward algorithm to compute the loglikelihood of a single observation sequence for an HMM.\n\n\n\n\n\nDensityInterface.logdensityof(hmm, obs_seqs, nb_seqs)\n\nApply the forward algorithm to compute the total loglikelihood of multiple observation sequences for an HMM\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.viterbi","page":"API reference","title":"HiddenMarkovModels.viterbi","text":"viterbi(hmm, obs_seq)\n\nApply the Viterbi algorithm to compute the most likely state sequence of an HMM for a single observation sequence.\n\n\n\n\n\nviterbi(hmm, obs_seqs, nb_seqs)\n\nApply the Viterbi algorithm to compute the most likely state sequences of an HMM for multiple observation sequences.\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.forward_backward","page":"API reference","title":"HiddenMarkovModels.forward_backward","text":"forward_backward(hmm, obs_seq)\n\nApply the forward-backward algorithm to estimate the posterior state marginals of an HMM for a single observation sequence.\n\n\n\n\n\nforward_backward(hmm, obs_seqs, nb_seqs)\n\nApply the forward-backward algorithm to estimate the posterior state marginals of an HMM for multiple observation sequences.\n\n\n\n\n\n","category":"function"},{"location":"api/#Learning","page":"API reference","title":"Learning","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"baum_welch","category":"page"},{"location":"api/#HiddenMarkovModels.baum_welch","page":"API reference","title":"HiddenMarkovModels.baum_welch","text":"baum_welch(hmm_init, obs_seq; max_iterations, rtol)\n\nApply the Baum-Welch algorithm to estimate the parameters of an HMM based on a single observation sequence, and return a tuple (hmm, logL_evolution).\n\n\n\n\n\nbaum_welch(hmm_init, obs_seqs, nb_seqs; max_iterations, rtol)\n\nApply the Baum-Welch algorithm to estimate the parameters of an HMM based on multiple observation sequences, and return a tuple (hmm, logL_evolution).\n\n\n\n\n\n","category":"function"},{"location":"api/#Reimplement-if-needed","page":"API reference","title":"Reimplement if needed","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"HMMs.fit!\nHMMs.fit_element_from_sequence!\nHMMs.LightDiagNormal","category":"page"},{"location":"api/#StatsAPI.fit!","page":"API reference","title":"StatsAPI.fit!","text":"StatsAPI.fit!(op::ObservationProcess, obs_seq, Î³)\n\nUpdate the initial distribution and transition matrix of sp based on weighted initialization counts p_count and transition counts A_count.\n\n\n\n\n\nStatsAPI.fit!(op::ObservationProcess, obs_seq, Î³)\n\nUpdate all observation distributions of op based on an observation sequence obs_seq, weighted by Î³[i, :] for each state i.\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.fit_element_from_sequence!","page":"API reference","title":"HiddenMarkovModels.fit_element_from_sequence!","text":"fit_element_from_sequence!(dists, i, x, w)\n\nModify the i-th element of dists by fitting it to an observation sequence x with associated weight sequence w.\n\nThe default behavior is a fallback on StatsAPI.fit!(dists[i], x, w), which users are encouraged to implement if their observation distributions are mutable. If not, check out the source code to see the hack we used for Distributions.jl, and imitate it.\n\n\n\n\n\n","category":"function"},{"location":"api/#HiddenMarkovModels.LightDiagNormal","page":"API reference","title":"HiddenMarkovModels.LightDiagNormal","text":"LightDiagonalNormal\n\nAn HMMs-compatible implementation of a multivariate normal distribution with diagonal covariance, enabling allocation-free estimation.\n\nThis is not part of the public API and is expected to change.\n\n\n\n\n\n","category":"type"},{"location":"api/#Index","page":"API reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"notations/#Notations","page":"Notations","title":"Notations","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"Our whole package is based on the following paper by Rabiner (1989):","category":"page"},{"location":"notations/","page":"Notations","title":"Notations","text":"A tutorial on hidden Markov models and selected applications in speech recognition","category":"page"},{"location":"notations/","page":"Notations","title":"Notations","text":"Please refer to it for mathematical explanations.","category":"page"},{"location":"notations/#Integers","page":"Notations","title":"Integers","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"N: number of states\nD: dimension of the observations\nT: trajectory length\nK: number of trajectories","category":"page"},{"location":"notations/#State-process","page":"Notations","title":"State process","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"sp or state_process: a StateProcess\np: initial_distribution (vector of state probabilities)\nA: transition_matrix (matrix of transition probabilities)\nstate_seq: a sequence of states (vector of integers)","category":"page"},{"location":"notations/#Observation-process","page":"Notations","title":"Observation process","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"op or obs_process: an ObservationProcess\n(log)b: vector of observation (log)likelihoods by state for an individual observation\n(log)B: matrix of observation (log)likelihoods by state for a sequence of observations\nobs_seq: a sequence of observations (vector of individual observations)\nobs_seqs: several sequences of observations","category":"page"},{"location":"notations/#Forward-backward","page":"Notations","title":"Forward backward","text":"","category":"section"},{"location":"notations/","page":"Notations","title":"Notations","text":"Î±: forward variables\nc: forward variable inverse normalizations\nÎ²: backward variables\nÎ³: one-state marginals\nÎ¾: two-state marginals\nlogL: loglikelihood of a sequence of observations","category":"page"},{"location":"#HiddenMarkovModels.jl","page":"Home","title":"HiddenMarkovModels.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package for HMM modeling, simulation, inference and learning.[1]","category":"page"},{"location":"","page":"Home","title":"Home","text":"[1]: Logo by ClÃ©ment Mantoux","category":"page"},{"location":"#Main-features","page":"Home","title":"Main features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"info: Performance\nallocation-free versions of core functions\nlinear algebra subroutines\nmultithreading\ncompatibility with StaticArrays.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Genericity\ntransition matrices can be dense or sparse\nobservations can be arbitrary Julia objects (not just numbers or arrays)\nemission distributions d must only implement rand(d) and logdensityof(d, x) (as per DensityInterface.jl)\npossibility to add priors\nnumber types are not restricted","category":"page"},{"location":"","page":"Home","title":"Home","text":"info: Automatic differentiation\nin forward mode with ForwardDiff.jl\nin reverse mode with ChainRules.jl (WIP)","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"warning: Work in progress\nIn the meantime, you can take a look at the files in test, which demonstrate more sophisticated ways to use the package.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using HiddenMarkovModels, Distributions","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Constructing an HMM:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"function random_gaussian_hmm(N)\n    p = ones(N) / N  # initial distribution\n    A = rand_trans_mat(N)  # transition matrix\n    dists = [Normal(randn(), 1.0) for n in 1:N]  # observation distributions\n    return HMM(p, A, dists)\nend;","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Checking its contents:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hmm = random_gaussian_hmm(3)\ntransition_matrix(hmm)\n[obs_distribution(hmm, i) for i in 1:length(hmm)]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Simulating a sequence:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"state_seq, obs_seq = rand(hmm, 1000);\nfirst(state_seq, 10)'\nfirst(obs_seq, 10)'","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Computing the loglikelihood of an observation sequence:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"logdensityof(hmm, obs_seq)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Inferring the most likely state sequence:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"most_likely_state_seq = viterbi(hmm, obs_seq);\nfirst(most_likely_state_seq, 10)'","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Learning the parameters based on an observation sequence:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"hmm_init = random_gaussian_hmm(3)\nhmm_est, logL_evolution = baum_welch(hmm_init, obs_seq);\nfirst(logL_evolution), last(logL_evolution)\ntransition_matrix(hmm_est)\n[obs_distribution(hmm_est, i) for i in 1:length(hmm)]","category":"page"}]
}
